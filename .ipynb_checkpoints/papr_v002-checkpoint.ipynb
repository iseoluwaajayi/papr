{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6dbd6c-db9e-4f61-a3fd-f115a5f13f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the method that uses the MATLAB Engine API for Python\n",
    "import matlab.engine\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torchvision import  models, datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import timm\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from scipy.io import savemat\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c9a3ac-fe84-4ef2-9b3b-1abc71872199",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9bcc4da-3c74-49b2-8398-587bd933337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be220735-ac4e-419e-b499-864df1d27ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_input = scio.loadmat('algorithm_input_.mat')\n",
    "algorithm_input_mat = algorithm_input['algorithm_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f6bc0b-09d2-4bb3-b8b7-7d577e69f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_output = scio.loadmat('algorithm_output_.mat')\n",
    "algorithm_output_mat = algorithm_output['algorithm_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ffcbe5c-99e0-43f9-b71b-2c0fdf5c6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_channels = scio.loadmat('main_channels_.mat')\n",
    "main_channels_mat = main_channels['main_channels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c30e94-9921-4ef9-9335-e913b68cf9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_store = scio.loadmat('symbols_store_.mat')\n",
    "symbols_store_mat = symbols_store['symbols_store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4dedec3-a4de-43cc-ba30-fb37b8d9a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat):\n",
    "        # convert into PyTorch tensors and remember them\n",
    "        self.algorithm_input_mat = algorithm_input_mat\n",
    "        self.algorithm_output_mat = algorithm_output_mat\n",
    "        self.main_channels_mat = main_channels_mat\n",
    "        self.symbols_store_mat = symbols_store_mat\n",
    "        \n",
    "    def __len__(self):\n",
    "        # this should return the size of the dataset\n",
    "        return len(self.algorithm_input_mat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # this should return one sample from the dataset\n",
    "        algorithm_input_mat = self.algorithm_input_mat[idx]\n",
    "        algorithm_output_mat = self.algorithm_output_mat[idx]\n",
    "        main_channels_mat = self.main_channels_mat[idx]\n",
    "        symbols_store_mat = self.symbols_store_mat[idx]\n",
    "        return algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf2a537d-4f72-4525-9d55-3f479405587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae911b39-4580-44fe-81de-864892143bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the dataset into train and remaining (val + test)\n",
    "train_set, remaining_set = train_test_split(dataset, test_size=0.25, random_state=42)\n",
    "\n",
    "# Now, split the remaining set into validation and test sets\n",
    "val_set, test_set = train_test_split(remaining_set, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47b372d6-28d2-4e1b-b2c7-4ecc3f46c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_set, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2425bdd-3cfc-4088-9f70-28db15772818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of batch feature is torch.Size([32, 70])\n",
      "shape of batch feature is torch.Size([32, 70])\n",
      "shape of batch feature is torch.Size([32, 10, 70])\n",
      "shape of batch feature is torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_alg_in_mat, batch_alg_out_mat, batch_main_chan_mat, batch_sym_mat = next(iter(train_loader))\n",
    "print(f'shape of batch feature is {batch_alg_in_mat.shape}')\n",
    "print(f'shape of batch feature is {batch_alg_out_mat.shape}')\n",
    "print(f'shape of batch feature is {batch_main_chan_mat.shape}')\n",
    "print(f'shape of batch feature is {batch_sym_mat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d61bc6fc-8fe1-48fe-bbae-8a8e38db11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelBased, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(140, 140)\n",
    "        self.bn1 = nn.BatchNorm1d(140)\n",
    "        \n",
    "        self.linear2 = nn.Linear(140, 280)\n",
    "        self.bn2 = nn.BatchNorm1d(280)\n",
    "        \n",
    "        self.linear3 = nn.Linear(280, 140)\n",
    "        self.bn3 = nn.BatchNorm1d(140)\n",
    "        \n",
    "        self.linear4 = nn.Linear(140, 140)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.linear1(x)))\n",
    "        x = F.relu(self.bn2(self.linear2(x)))\n",
    "        x = F.relu(self.bn3(self.linear3(x)))\n",
    "        x = self.linear4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "061bae05-f1a6-43e0-a565-b1ee340d4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelBased().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d878603-b262-4329-9d66-695bcdf7e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.rand([32,140]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7313cc0-044d-4bf8-bb24-c2c9fef1584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12ed62e2-e93d-4127-9bf5-50b5f7765684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 140])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43c773cf-8dc6-4543-afb3-67041ab61a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_shape = ModelBased()(torch.rand([32,140])).dtype\n",
    "test_output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e0a58e-4083-4f92-a375-da27fcd11eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_to_interleaved_real(complex_signal):\n",
    "    real_part = complex_signal.real.to(dtype=torch.float32) \n",
    "    imag_part = complex_signal.imag.to(dtype=torch.float32) \n",
    "    interleaved_signal = torch.stack((real_part, imag_part), dim=2).reshape(complex_signal.shape[0], -1)\n",
    "    return interleaved_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa1c6bd0-3830-41ee-8f79-168a01a7530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleaved_real_to_complex(interleaved_signal):\n",
    "    signal_length = interleaved_signal.shape[1] // 2\n",
    "    real_part = interleaved_signal[:, 0::2]  # Extract even indices\n",
    "    imag_part = interleaved_signal[:, 1::2]  # Extract odd indices\n",
    "    complex_signal = torch.complex(real_part, imag_part)\n",
    "    return complex_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d85abef5-3c5a-474f-aee1-1761ae32f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_papr_complex(signal):\n",
    "    # Compute |x[n]|^2: Magnitude squared of the complex signal\n",
    "    power_signal = torch.abs(signal)**2\n",
    "    \n",
    "    # Peak power\n",
    "    peak_power_signal= torch.max(power_signal, dim=1).values\n",
    "\n",
    "    # Average power\n",
    "    avg_power_signal = torch.mean(power_signal, dim=1)\n",
    "\n",
    "    # PAPR\n",
    "    papr_signal = peak_power_signal / avg_power_signal\n",
    "    \n",
    "    return papr_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b9bb3f2-4dc6-4155-a8da-627445761686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def papr_loss(signal_going_out, signal_coming_in):\n",
    "    # Compute PAPR before and after\n",
    "    papr_going_out = compute_papr_complex(signal_going_out)  # Transformed signal\n",
    "    papr_coming_in = compute_papr_complex(signal_coming_in)  # Original signal\n",
    "\n",
    "    # Penalize only if PAPR after is greater than PAPR before\n",
    "    papr_diff = torch.relu(papr_going_out - 0.5*papr_coming_in)\n",
    "    \n",
    "    return torch.mean(papr_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1d9eb3a-3856-4452-805e-ca119bfacdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_matlab(batch_alg_in_mat, batch_alg_out_mat, batch_nn_out, batch_main_chan_mat, batch_sym_mat):\n",
    "    \n",
    "    batch_alg_in_mat_real = matlab.double(batch_alg_in_mat.real.tolist())\n",
    "    batch_alg_in_mat_imag = matlab.double(batch_alg_in_mat.imag.tolist())\n",
    "\n",
    "    batch_alg_out_mat_real = matlab.double(batch_alg_out_mat.real.tolist())\n",
    "    batch_alg_out_mat_imag = matlab.double(batch_alg_out_mat.imag.tolist())\n",
    "\n",
    "    batch_nn_out_real = matlab.double(batch_nn_out.real.tolist())\n",
    "    batch_nn_out_imag = matlab.double(batch_nn_out.imag.tolist())\n",
    "\n",
    "    batch_main_chan_mat_real = matlab.double(batch_main_chan_mat.real.tolist())\n",
    "    batch_main_chan_mat_imag = matlab.double(batch_main_chan_mat.imag.tolist())\n",
    "\n",
    "    batch_sym_mat = matlab.uint8(batch_sym_mat.tolist())\n",
    "\n",
    "    return batch_alg_in_mat_real, batch_alg_in_mat_imag, batch_alg_out_mat_real, batch_alg_out_mat_imag, batch_nn_out_real, batch_nn_out_imag, batch_main_chan_mat_real, batch_main_chan_mat_imag, batch_sym_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c00e4fa-534d-4ae9-977d-c5cca46a7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ser_loss(batch_alg_in_mat, batch_alg_out_mat, batch_nn_out, batch_main_chan_mat, batch_sym_mat):\n",
    "\n",
    "    batch_alg_in_mat_real, batch_alg_in_mat_imag, batch_alg_out_mat_real, batch_alg_out_mat_imag, batch_nn_out_real, batch_nn_out_imag, batch_main_chan_mat_real, batch_main_chan_mat_imag , batch_sym_mat = prepare_for_matlab(batch_alg_in_mat, batch_alg_out_mat, batch_nn_out, batch_main_chan_mat, batch_sym_mat)\n",
    "    ser_mat = eng.calculate_ser(batch_alg_in_mat_real, batch_alg_in_mat_imag, batch_alg_out_mat_real, batch_alg_out_mat_imag, batch_nn_out_real, batch_nn_out_imag, batch_main_chan_mat_real, batch_main_chan_mat_imag , batch_sym_mat)\n",
    "    ser_torch = torch.tensor(ser_mat, dtype=torch.float32)\n",
    "    ser_diff = torch.relu(ser_torch[:,2] - ser_torch[:,1])\n",
    "    return torch.mean(ser_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94ced050-fea9-4688-85db-309ea7b31a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch [1/2] V Loss:0.0183 PAPR_dff: 0.000000 SER_dff: 0.1750: 100%|█| 391/391 [00:02<00:00, 142.36it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.0183 at Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch [2/2] V Loss:0.0083 PAPR_dff: 0.000000 SER_dff: 0.1500: 100%|█| 391/391 [00:02<00:00, 146.69it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.0083 at Epoch: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelBased().to(device)\n",
    "\n",
    "# Define the loss functions\n",
    "loss = torch.nn.MSELoss()  # For classification\n",
    "\n",
    "# Define an optimizer (both for the encoder and the decoder!)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.01)  # Learning rate decay scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=2)\n",
    "\n",
    "# Variables for early stopping and best parameters\n",
    "best_loss = float('inf')\n",
    "patience_limit = 10\n",
    "\n",
    "\n",
    "best_model = None\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "gamma = 1\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    progress_bar_train = tqdm(enumerate(train_loader), total=len(train_loader), ncols=100, leave=True)\n",
    "    for index, (algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat) in progress_bar_train:\n",
    "        # Forward pass\n",
    "        algorithm_output_mat_for_nn = complex_to_interleaved_real(algorithm_output_mat)\n",
    "        algorithm_output_mat_for_nn = algorithm_output_mat_for_nn.to(device)\n",
    "        \n",
    "        nn_output = model(algorithm_output_mat_for_nn)\n",
    "        \n",
    "        # Calculate loss\n",
    "        initial_loss = loss(nn_output, algorithm_output_mat_for_nn)\n",
    "\n",
    "        nn_output_control =  interleaved_real_to_complex(nn_output)\n",
    "        \n",
    "        papr_diff = papr_loss(nn_output_control, algorithm_output_mat_for_nn)\n",
    "        ser_diff = ser_loss(algorithm_input_mat, algorithm_output_mat, nn_output_control, main_channels_mat, symbols_store_mat)\n",
    "        \n",
    "        train_loss = alpha*initial_loss + beta*papr_diff + gamma*ser_diff\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_train_loss += train_loss.item()\n",
    "        avg_train_loss = running_train_loss / (index + 1)\n",
    "\n",
    "        # Print metrics\n",
    "        #progress_bar_train.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] MSELos:{avg_train_loss1:.4f} MSEWeig{mse_weight:.2f} CELos:{avg_train_loss2:.4f} CEWeig{ce_weight:.2f} TrLos:{avg_train_loss:.4f} Tr.Acc: {avg_train_acc*100:.2f}%')\n",
    "        progress_bar_train.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] T Loss:{avg_train_loss:.4f} PAPR_dff: {papr_diff:.6f} SER_dff: {ser_diff:.4f}')\n",
    "    \n",
    "    #train_losses.append(avg_train_loss)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    progress_bar_val = tqdm(enumerate(val_loader), total=len(val_loader), ncols=100, leave=True)\n",
    "    for index, (algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat) in progress_bar_val:\n",
    "        \n",
    "        algorithm_output_mat_for_nn = complex_to_interleaved_real(algorithm_output_mat)\n",
    "        algorithm_output_mat_for_nn = algorithm_output_mat_for_nn.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            nn_output = model(algorithm_output_mat_for_nn)\n",
    "\n",
    "            # Calculate losses\n",
    "            val_loss = loss(nn_output, algorithm_output_mat_for_nn)\n",
    "\n",
    "            # Update running loss\n",
    "            running_val_loss += val_loss.item()\n",
    "            \n",
    "            avg_val_loss = running_val_loss / (index + 1)\n",
    "\n",
    "            nn_output_control =  interleaved_real_to_complex(nn_output)\n",
    "        \n",
    "            papr_diff = papr_loss(nn_output_control, algorithm_output_mat_for_nn)\n",
    "            ser_diff = ser_loss(algorithm_input_mat, algorithm_output_mat, nn_output_control, main_channels_mat, symbols_store_mat)\n",
    "\n",
    "            progress_bar_val.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] V Loss:{avg_val_loss:.4f} PAPR_dff: {papr_diff:.6f} SER_dff: {ser_diff:.4f}')\n",
    "    \n",
    "    #val_losses.append(avg_val_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    scheduler.step(running_val_loss)\n",
    "\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:  # Now checking for the best accuracy\n",
    "        best_loss = avg_val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        best_train_loss = avg_train_loss\n",
    "        patience_ = 0\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "        print(f\"Best Validation Loss is now: {best_loss:.4f} at Epoch: {best_epoch}\")\n",
    "    else:\n",
    "        patience_ += 1\n",
    "        print(f\"This is Epoch: {patience_} without improvement\")\n",
    "        print(f\"Current Validation Loss is: {avg_val_loss:.4f} at Epoch: {epoch+1}\")\n",
    "        print(f\"Best Validation Loss remains: {best_loss:.4f} at Epoch: {best_epoch}\")\n",
    "        if patience_ > patience_limit:  # Patience limit before stopping\n",
    "            print(\"Early stopping triggered! Restoring best model weights.\")\n",
    "            print(f\"Best Validation Loss was: {best_loss:.4f} at Epoch: {best_epoch}\")\n",
    "            break\n",
    "\n",
    "best_model = model.cpu()\n",
    "best_model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "830b9432-9787-4c17-ab5c-d7d6378506d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch [2/2] Te Loss:0.0086 PAPR_dff: 0.0000 SER_dff: 0.0050: 100%|█| 391/391 [00:05<00:00, 71.77it/s"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "running_test_loss = 0.0\n",
    "\n",
    "\n",
    "progress_bar_test = tqdm(enumerate(test_loader), total=len(test_loader), ncols=100, leave=True)\n",
    "for index, (algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat) in progress_bar_test:\n",
    "        \n",
    "    algorithm_output_mat_for_nn = complex_to_interleaved_real(algorithm_output_mat)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            \n",
    "        nn_output = best_model(algorithm_output_mat_for_nn)\n",
    "\n",
    "        # Calculate losses\n",
    "        test_loss = loss(nn_output, algorithm_output_mat_for_nn)\n",
    "\n",
    "        # Update running loss\n",
    "        running_test_loss += test_loss.item()\n",
    "            \n",
    "        avg_test_loss = running_test_loss / (index + 1)\n",
    "\n",
    "        nn_output_control =  interleaved_real_to_complex(nn_output)\n",
    "        \n",
    "        papr_diff = papr_loss(nn_output_control, algorithm_output_mat_for_nn)\n",
    "        ser_diff = ser_loss(algorithm_input_mat, algorithm_output_mat, nn_output_control, main_channels_mat, symbols_store_mat)\n",
    "\n",
    "        progress_bar_test.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] Te Loss:{avg_test_loss:.4f} PAPR_dff: {papr_diff:.4f} SER_dff: {ser_diff:.4f}')\n",
    "\n",
    "        \n",
    "        if index < 1:\n",
    "            total_nn_out = nn_output\n",
    "            total_alg_in = algorithm_input_mat\n",
    "            total_alg_out = algorithm_output_mat\n",
    "            total_main_channels = main_channels_mat\n",
    "            total_symbols = symbols_store_mat\n",
    "        else:\n",
    "            total_nn_out = torch.cat([total_nn_out, nn_output], dim=0, out=None)\n",
    "            total_alg_in = torch.cat([total_alg_in, algorithm_input_mat], dim=0, out=None)\n",
    "            total_alg_out = torch.cat([total_alg_out, algorithm_output_mat], dim=0, out=None)\n",
    "            total_main_channels = torch.cat([total_main_channels, main_channels_mat], dim=0, out=None)\n",
    "            total_symbols = torch.cat([total_symbols, symbols_store_mat], dim=0, out=None)\n",
    "\n",
    "\n",
    "        progress_bar_val.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] Te Loss:{avg_test_loss:.4f} PAPR_dff: {papr_diff:.4f} SER_dff: {ser_diff:.4f}')\n",
    "    \n",
    "test_losses.append(avg_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66f020a4-b573-4f70-8d66-dab90fc0de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_alg_in_real, total_alg_in_imag, total_alg_out_real, total_alg_out_imag, total_nn_out_real, total_nn_out_imag, total_main_channels_real, total_main_channels_imag, total_symbols = prepare_for_matlab(total_alg_in, total_alg_out, total_nn_out, total_main_channels, total_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f0d677f-c655-4a98-9636-1955d4cdc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all variables in a dictionary\n",
    "savemat(\"output_from_pytorch.mat\", {\n",
    "    \"total_alg_in_real\": total_alg_in_real,\n",
    "    \"total_alg_in_imag\": total_alg_in_imag,\n",
    "    \"total_alg_out_real\": total_alg_out_real,\n",
    "    \"total_alg_out_imag\": total_alg_out_imag,\n",
    "    \"total_nn_out_real\": total_nn_out_real,\n",
    "    \"total_nn_out_imag\": total_nn_out_imag,\n",
    "    \"total_main_channels_real\": total_main_channels_real,\n",
    "    \"total_main_channels_imag\": total_main_channels_imag,\n",
    "    \"total_symbols\": total_symbols\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e49995-da2d-4b44-ad81-59217a411746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
