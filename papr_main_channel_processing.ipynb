{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6dbd6c-db9e-4f61-a3fd-f115a5f13f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the method that uses the MATLAB Engine API for Python\n",
    "import matlab.engine\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torchvision import  models, datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import timm\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from scipy.io import savemat\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c9a3ac-fe84-4ef2-9b3b-1abc71872199",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9bcc4da-3c74-49b2-8398-587bd933337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be220735-ac4e-419e-b499-864df1d27ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_input = scio.loadmat('algorithm_input.mat')\n",
    "algorithm_input_mat = algorithm_input['algorithm_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38843456-8180-4549-8b71-821a84449721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700000, 70)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm_input_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f6bc0b-09d2-4bb3-b8b7-7d577e69f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_output = scio.loadmat('algorithm_output.mat')\n",
    "algorithm_output_mat = algorithm_output['algorithm_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2cb8f64-2ced-42c8-88be-36b6db4b002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700000, 70)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm_output_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c32778-38e9-4b3f-8a2e-76d6980c1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_channels = h5py.File('main_channels.mat', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa1e63c4-2b40-47d1-a392-4517920a95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_channels_data = main_channels['main_channels'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86737f5b-87b1-4b69-a854-13301212ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_part = main_channels_data['real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "353231a2-a7b6-4f1e-9aae-651cb54b1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_part_tensor = torch.tensor(real_part, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9aa2897-5f5b-45f1-a8c1-e58091f47367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 10, 700000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_part_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d247014-9894-42c1-a537-8e7581ede93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imag_part = main_channels_data['imag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3427bf1b-0f29-45a4-b0e5-845fcd443004",
   "metadata": {},
   "outputs": [],
   "source": [
    "imag_part_tensor = torch.tensor(imag_part, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c0c024b-1446-4979-8b4b-bdaca0cc20a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 10, 700000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imag_part_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53ac5a18-f5ee-4e48-b9e6-26fe0496c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_channels_tensor = torch.complex(real_part_tensor, imag_part_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6f6c714-67c1-41d6-9d41-29ac75589314",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_channels_tensor = main_channels_tensor.permute(2,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "191cf877-493f-4012-928a-e373d47a2939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700000, 10, 70])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_channels_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4630df5d-6dd2-448e-b297-8b0eee29bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(main_channels_tensor,'main_channels_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e234ddf-f46b-411a-98a1-c4f0f702e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_channels = torch.load('main_channels_tensor.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0d4b196-d18d-459d-9c39-ae78f519d9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700000, 10, 70])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_channels_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d594812a-cb27-49e5-b08f-a2cc07bfb028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del main_channels_tensor\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18da5fcb-6187-4c7b-9b1e-5196a38114ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(main_channels_tensor,'main_channels_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb3b0f30-6985-4fb3-89fa-d53ceb99acb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980000000,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ee262e1-1f72-48ac-ab19-5583171269ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.tensor(combined, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ee839b9-9eca-45b7-838e-eea88e15bb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([980000000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc22613-9ead-4ba9-89c2-25d70c1556cf",
   "metadata": {},
   "source": [
    "main_channels = scio.loadmat('main_channels.mat')\n",
    "main_channels_mat = main_channels['main_channels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3c30e94-9921-4ef9-9335-e913b68cf9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_store = scio.loadmat('symbols_store.mat')\n",
    "symbols_store_mat = symbols_store['symbols_store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4dedec3-a4de-43cc-ba30-fb37b8d9a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat):\n",
    "        # convert into PyTorch tensors and remember them\n",
    "        self.algorithm_input_mat = algorithm_input_mat\n",
    "        self.algorithm_output_mat = algorithm_output_mat\n",
    "        self.main_channels_mat = main_channels_mat\n",
    "        self.symbols_store_mat = symbols_store_mat\n",
    "        \n",
    "    def __len__(self):\n",
    "        # this should return the size of the dataset\n",
    "        return len(self.algorithm_input_mat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # this should return one sample from the dataset\n",
    "        algorithm_input_mat = self.algorithm_input_mat[idx]\n",
    "        algorithm_output_mat = self.algorithm_output_mat[idx]\n",
    "        main_channels_mat = self.main_channels_mat[:,:,idx]\n",
    "        symbols_store_mat = self.symbols_store_mat[idx]\n",
    "        return algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf2a537d-4f72-4525-9d55-3f479405587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae911b39-4580-44fe-81de-864892143bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the dataset into train and remaining (val + test)\n",
    "train_set, remaining_set = train_test_split(dataset, test_size=200000, random_state=42)\n",
    "\n",
    "# Now, split the remaining set into validation and test sets\n",
    "val_set, test_set = train_test_split(remaining_set, test_size=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47b372d6-28d2-4e1b-b2c7-4ecc3f46c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_set, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2425bdd-3cfc-4088-9f70-28db15772818",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.void. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_alg_in_mat, batch_alg_out_mat, batch_main_chan_mat, batch_sym_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:285\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.void. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "batch_alg_in_mat, batch_alg_out_mat, batch_main_chan_mat, batch_sym_mat = next(iter(train_loader))\n",
    "print(f'shape of batch feature is {batch_alg_in_mat.shape}')\n",
    "print(f'shape of batch feature is {batch_alg_out_mat.shape}')\n",
    "print(f'shape of batch feature is {batch_main_chan_mat.shape}')\n",
    "print(f'shape of batch feature is {batch_sym_mat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d61bc6fc-8fe1-48fe-bbae-8a8e38db11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelBased, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(140, 140)\n",
    "        self.bn1 = nn.BatchNorm1d(140)\n",
    "        \n",
    "        self.linear2 = nn.Linear(140, 280)\n",
    "        self.bn2 = nn.BatchNorm1d(280)\n",
    "        \n",
    "        self.linear3 = nn.Linear(280, 140)\n",
    "        self.bn3 = nn.BatchNorm1d(140)\n",
    "        \n",
    "        self.linear4 = nn.Linear(140, 140)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.linear1(x)))\n",
    "        x = F.relu(self.bn2(self.linear2(x)))\n",
    "        x = F.relu(self.bn3(self.linear3(x)))\n",
    "        x = self.linear4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "061bae05-f1a6-43e0-a565-b1ee340d4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelBased().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d878603-b262-4329-9d66-695bcdf7e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.rand([32,140]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7313cc0-044d-4bf8-bb24-c2c9fef1584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12ed62e2-e93d-4127-9bf5-50b5f7765684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 140])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43c773cf-8dc6-4543-afb3-67041ab61a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_shape = ModelBased()(torch.rand([32,140])).dtype\n",
    "test_output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e0a58e-4083-4f92-a375-da27fcd11eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_to_interleaved_real(complex_signal):\n",
    "    real_part = complex_signal.real.to(dtype=torch.float32) \n",
    "    imag_part = complex_signal.imag.to(dtype=torch.float32) \n",
    "    interleaved_signal = torch.stack((real_part, imag_part), dim=2).reshape(complex_signal.shape[0], -1)\n",
    "    return interleaved_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa1c6bd0-3830-41ee-8f79-168a01a7530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleaved_real_to_complex(interleaved_signal):\n",
    "    signal_length = interleaved_signal.shape[1] // 2\n",
    "    real_part = interleaved_signal[:, 0::2]  # Extract even indices\n",
    "    imag_part = interleaved_signal[:, 1::2]  # Extract odd indices\n",
    "    complex_signal = torch.complex(real_part, imag_part)\n",
    "    return complex_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d85abef5-3c5a-474f-aee1-1761ae32f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_papr_complex(signal):\n",
    "    # Compute |x[n]|^2: Magnitude squared of the complex signal\n",
    "    power_signal = torch.abs(signal)**2\n",
    "    \n",
    "    # Peak power\n",
    "    peak_power_signal= torch.max(power_signal, dim=1).values\n",
    "\n",
    "    # Average power\n",
    "    avg_power_signal = torch.mean(power_signal, dim=1)\n",
    "\n",
    "    # PAPR\n",
    "    papr_signal = peak_power_signal / avg_power_signal\n",
    "    \n",
    "    return papr_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b9bb3f2-4dc6-4155-a8da-627445761686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def papr_loss(signal_going_out, signal_coming_in):\n",
    "    # Compute PAPR before and after\n",
    "    papr_going_out = compute_papr_complex(signal_going_out)  # Transformed signal\n",
    "    papr_coming_in = compute_papr_complex(signal_coming_in)  # Original signal\n",
    "\n",
    "    # Penalize only if PAPR after is greater than PAPR before\n",
    "    papr_diff = torch.relu(papr_going_out - 0.5*papr_coming_in)\n",
    "    \n",
    "    return torch.mean(papr_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1d9eb3a-3856-4452-805e-ca119bfacdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_matlab(batch_alg_in_mat, batch_alg_out_mat, batch_nn_out, batch_main_chan_mat, batch_sym_mat):\n",
    "    \n",
    "    batch_alg_in_mat_real = matlab.double(batch_alg_in_mat.real.tolist())\n",
    "    batch_alg_in_mat_imag = matlab.double(batch_alg_in_mat.imag.tolist())\n",
    "\n",
    "    batch_alg_out_mat_real = matlab.double(batch_alg_out_mat.real.tolist())\n",
    "    batch_alg_out_mat_imag = matlab.double(batch_alg_out_mat.imag.tolist())\n",
    "\n",
    "    batch_nn_out_real = matlab.double(batch_nn_out.real.tolist())\n",
    "    batch_nn_out_imag = matlab.double(batch_nn_out.imag.tolist())\n",
    "\n",
    "    batch_main_chan_mat_real = matlab.double(batch_main_chan_mat.real.tolist())\n",
    "    batch_main_chan_mat_imag = matlab.double(batch_main_chan_mat.imag.tolist())\n",
    "\n",
    "    batch_sym_mat = matlab.uint32(batch_sym_mat.tolist())\n",
    "\n",
    "    return batch_alg_in_mat_real, batch_alg_in_mat_imag, batch_alg_out_mat_real, batch_alg_out_mat_imag, batch_nn_out_real, batch_nn_out_imag, batch_main_chan_mat_real, batch_main_chan_mat_imag, batch_sym_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c00e4fa-534d-4ae9-977d-c5cca46a7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ser_loss(batch_alg_in_mat, batch_alg_out_mat, batch_nn_out, batch_main_chan_mat, batch_sym_mat):\n",
    "\n",
    "    batch_alg_in_mat_real, batch_alg_in_mat_imag, batch_alg_out_mat_real, batch_alg_out_mat_imag, batch_nn_out_real, batch_nn_out_imag, batch_main_chan_mat_real, batch_main_chan_mat_imag , batch_sym_mat = prepare_for_matlab(batch_alg_in_mat, batch_alg_out_mat, batch_nn_out, batch_main_chan_mat, batch_sym_mat)\n",
    "    ser_mat = eng.calculate_ser(batch_alg_in_mat_real, batch_alg_in_mat_imag, batch_alg_out_mat_real, batch_alg_out_mat_imag, batch_nn_out_real, batch_nn_out_imag, batch_main_chan_mat_real, batch_main_chan_mat_imag , batch_sym_mat)\n",
    "    ser_torch = torch.tensor(ser_mat, dtype=torch.float32)\n",
    "    ser_diff = torch.relu(ser_torch[:,2] - ser_torch[:,1])\n",
    "    return torch.mean(ser_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94ced050-fea9-4688-85db-309ea7b31a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch [1/2] V Loss:0.0183 PAPR_dff: 0.000000 SER_dff: 0.1750: 100%|█| 391/391 [00:02<00:00, 142.36it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.0183 at Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch [2/2] V Loss:0.0083 PAPR_dff: 0.000000 SER_dff: 0.1500: 100%|█| 391/391 [00:02<00:00, 146.69it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.0083 at Epoch: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelBased().to(device)\n",
    "\n",
    "# Define the loss functions\n",
    "loss = torch.nn.MSELoss()  # For classification\n",
    "\n",
    "# Define an optimizer (both for the encoder and the decoder!)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.01)  # Learning rate decay scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=2)\n",
    "\n",
    "# Variables for early stopping and best parameters\n",
    "best_loss = float('inf')\n",
    "patience_limit = 10\n",
    "\n",
    "\n",
    "best_model = None\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "gamma = 1\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    progress_bar_train = tqdm(enumerate(train_loader), total=len(train_loader), ncols=100, leave=True)\n",
    "    for index, (algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat) in progress_bar_train:\n",
    "        # Forward pass\n",
    "        algorithm_output_mat_for_nn = complex_to_interleaved_real(algorithm_output_mat)\n",
    "        algorithm_output_mat_for_nn = algorithm_output_mat_for_nn.to(device)\n",
    "        \n",
    "        nn_output = model(algorithm_output_mat_for_nn)\n",
    "        \n",
    "        # Calculate loss\n",
    "        initial_loss = loss(nn_output, algorithm_output_mat_for_nn)\n",
    "\n",
    "        nn_output_control =  interleaved_real_to_complex(nn_output)\n",
    "        \n",
    "        papr_diff = papr_loss(nn_output_control, algorithm_output_mat_for_nn)\n",
    "        ser_diff = ser_loss(algorithm_input_mat, algorithm_output_mat, nn_output_control, main_channels_mat, symbols_store_mat)\n",
    "        \n",
    "        train_loss = alpha*initial_loss + beta*papr_diff + gamma*ser_diff\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_train_loss += train_loss.item()\n",
    "        avg_train_loss = running_train_loss / (index + 1)\n",
    "\n",
    "        # Print metrics\n",
    "        #progress_bar_train.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] MSELos:{avg_train_loss1:.4f} MSEWeig{mse_weight:.2f} CELos:{avg_train_loss2:.4f} CEWeig{ce_weight:.2f} TrLos:{avg_train_loss:.4f} Tr.Acc: {avg_train_acc*100:.2f}%')\n",
    "        progress_bar_train.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] T Loss:{avg_train_loss:.4f} PAPR_dff: {papr_diff:.6f} SER_dff: {ser_diff:.4f}')\n",
    "    \n",
    "    #train_losses.append(avg_train_loss)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    progress_bar_val = tqdm(enumerate(val_loader), total=len(val_loader), ncols=100, leave=True)\n",
    "    for index, (algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat) in progress_bar_val:\n",
    "        \n",
    "        algorithm_output_mat_for_nn = complex_to_interleaved_real(algorithm_output_mat)\n",
    "        algorithm_output_mat_for_nn = algorithm_output_mat_for_nn.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            nn_output = model(algorithm_output_mat_for_nn)\n",
    "\n",
    "            # Calculate losses\n",
    "            val_loss = loss(nn_output, algorithm_output_mat_for_nn)\n",
    "\n",
    "            # Update running loss\n",
    "            running_val_loss += val_loss.item()\n",
    "            \n",
    "            avg_val_loss = running_val_loss / (index + 1)\n",
    "\n",
    "            nn_output_control =  interleaved_real_to_complex(nn_output)\n",
    "        \n",
    "            papr_diff = papr_loss(nn_output_control, algorithm_output_mat_for_nn)\n",
    "            ser_diff = ser_loss(algorithm_input_mat, algorithm_output_mat, nn_output_control, main_channels_mat, symbols_store_mat)\n",
    "\n",
    "            progress_bar_val.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] V Loss:{avg_val_loss:.4f} PAPR_dff: {papr_diff:.6f} SER_dff: {ser_diff:.4f}')\n",
    "    \n",
    "    #val_losses.append(avg_val_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    scheduler.step(running_val_loss)\n",
    "\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:  # Now checking for the best accuracy\n",
    "        best_loss = avg_val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        best_train_loss = avg_train_loss\n",
    "        patience_ = 0\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "        print(f\"Best Validation Loss is now: {best_loss:.4f} at Epoch: {best_epoch}\")\n",
    "    else:\n",
    "        patience_ += 1\n",
    "        print(f\"This is Epoch: {patience_} without improvement\")\n",
    "        print(f\"Current Validation Loss is: {avg_val_loss:.4f} at Epoch: {epoch+1}\")\n",
    "        print(f\"Best Validation Loss remains: {best_loss:.4f} at Epoch: {best_epoch}\")\n",
    "        if patience_ > patience_limit:  # Patience limit before stopping\n",
    "            print(\"Early stopping triggered! Restoring best model weights.\")\n",
    "            print(f\"Best Validation Loss was: {best_loss:.4f} at Epoch: {best_epoch}\")\n",
    "            break\n",
    "\n",
    "best_model = model.cpu()\n",
    "best_model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "830b9432-9787-4c17-ab5c-d7d6378506d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch [2/2] Te Loss:0.0086 PAPR_dff: 0.0000 SER_dff: 0.0050: 100%|█| 391/391 [00:05<00:00, 71.77it/s"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "running_test_loss = 0.0\n",
    "\n",
    "\n",
    "progress_bar_test = tqdm(enumerate(test_loader), total=len(test_loader), ncols=100, leave=True)\n",
    "for index, (algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat) in progress_bar_test:\n",
    "        \n",
    "    algorithm_output_mat_for_nn = complex_to_interleaved_real(algorithm_output_mat)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            \n",
    "        nn_output = best_model(algorithm_output_mat_for_nn)\n",
    "\n",
    "        # Calculate losses\n",
    "        test_loss = loss(nn_output, algorithm_output_mat_for_nn)\n",
    "\n",
    "        # Update running loss\n",
    "        running_test_loss += test_loss.item()\n",
    "            \n",
    "        avg_test_loss = running_test_loss / (index + 1)\n",
    "\n",
    "        nn_output_control =  interleaved_real_to_complex(nn_output)\n",
    "        \n",
    "        papr_diff = papr_loss(nn_output_control, algorithm_output_mat_for_nn)\n",
    "        ser_diff = ser_loss(algorithm_input_mat, algorithm_output_mat, nn_output_control, main_channels_mat, symbols_store_mat)\n",
    "\n",
    "        progress_bar_test.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] Te Loss:{avg_test_loss:.4f} PAPR_dff: {papr_diff:.4f} SER_dff: {ser_diff:.4f}')\n",
    "\n",
    "        \n",
    "        if index < 1:\n",
    "            total_nn_out = nn_output\n",
    "            total_alg_in = algorithm_input_mat\n",
    "            total_alg_out = algorithm_output_mat\n",
    "            total_main_channels = main_channels_mat\n",
    "            total_symbols = symbols_store_mat\n",
    "        else:\n",
    "            total_nn_out = torch.cat([total_nn_out, nn_output], dim=0, out=None)\n",
    "            total_alg_in = torch.cat([total_alg_in, algorithm_input_mat], dim=0, out=None)\n",
    "            total_alg_out = torch.cat([total_alg_out, algorithm_output_mat], dim=0, out=None)\n",
    "            total_main_channels = torch.cat([total_main_channels, main_channels_mat], dim=0, out=None)\n",
    "            total_symbols = torch.cat([total_symbols, symbols_store_mat], dim=0, out=None)\n",
    "\n",
    "\n",
    "        progress_bar_val.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] Te Loss:{avg_test_loss:.4f} PAPR_dff: {papr_diff:.4f} SER_dff: {ser_diff:.4f}')\n",
    "    \n",
    "test_losses.append(avg_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66f020a4-b573-4f70-8d66-dab90fc0de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_alg_in_real, total_alg_in_imag, total_alg_out_real, total_alg_out_imag, total_nn_out_real, total_nn_out_imag, total_main_channels_real, total_main_channels_imag, total_symbols = prepare_for_matlab(total_alg_in, total_alg_out, total_nn_out, total_main_channels, total_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f0d677f-c655-4a98-9636-1955d4cdc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all variables in a dictionary\n",
    "savemat(\"output_from_pytorch.mat\", {\n",
    "    \"total_alg_in_real\": total_alg_in_real,\n",
    "    \"total_alg_in_imag\": total_alg_in_imag,\n",
    "    \"total_alg_out_real\": total_alg_out_real,\n",
    "    \"total_alg_out_imag\": total_alg_out_imag,\n",
    "    \"total_nn_out_real\": total_nn_out_real,\n",
    "    \"total_nn_out_imag\": total_nn_out_imag,\n",
    "    \"total_main_channels_real\": total_main_channels_real,\n",
    "    \"total_main_channels_imag\": total_main_channels_imag,\n",
    "    \"total_symbols\": total_symbols\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e49995-da2d-4b44-ad81-59217a411746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
