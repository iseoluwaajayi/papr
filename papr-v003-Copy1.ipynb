{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6dbd6c-db9e-4f61-a3fd-f115a5f13f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the method that uses the MATLAB Engine API for Python\n",
    "import matlab.engine\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torchvision import  models, datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import timm\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from scipy.io import savemat\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c9a3ac-fe84-4ef2-9b3b-1abc71872199",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9bcc4da-3c74-49b2-8398-587bd933337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be220735-ac4e-419e-b499-864df1d27ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_input = scio.loadmat('algorithm_input_single.mat')\n",
    "algorithm_input_mat = algorithm_input['algorithm_input']\n",
    "algorithm_input_torch = torch.from_numpy(algorithm_input_mat.astype(np.complex64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea39553d-7cb8-431f-a554-2f1370b9c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datatype of the algorithm_input is torch.complex64\n"
     ]
    }
   ],
   "source": [
    "print(f\"The datatype of the algorithm_input is {algorithm_input_torch.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f41c6591-ceb7-45a1-9300-c38953422a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the algorithm_input is torch.Size([100000, 70])\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the algorithm_input is {algorithm_input_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f6bc0b-09d2-4bb3-b8b7-7d577e69f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_output = scio.loadmat('algorithm_output_single.mat')\n",
    "algorithm_output_mat = algorithm_output['algorithm_output']\n",
    "algorithm_output_torch = torch.from_numpy(algorithm_output_mat.astype(np.complex64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a101284-4c7e-448f-86aa-729486ba58a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of the algorithm_output is torch.complex64\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data type of the algorithm_output is {algorithm_output_torch.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482f824e-f692-4c41-a3ac-7d9e8062d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the algorithm_output is torch.Size([100000, 70])\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the algorithm_output is {algorithm_output_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d63575-44fd-4b1d-9cde-c7c6606d1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_channels = scio.loadmat('main_channels_single.mat')\n",
    "main_channels_mat = main_channels['main_channels']\n",
    "main_channels_torch = torch.from_numpy(main_channels_mat.astype(np.complex64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d6c2b0d-a04a-483f-a3c5-aeb31b5ef7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of the main_channels is torch.complex64\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data type of the main_channels is {main_channels_torch.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e21c0895-4131-4e12-8068-b0f3c7d61da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the main_channels is torch.Size([100000, 10, 70])\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the main_channels is {main_channels_torch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0c9eb-37ac-4bae-9dd2-cc99724162be",
   "metadata": {},
   "source": [
    "main_channels_mat = torch.load('main_channels_tensor.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55bb26fa-9676-46ad-9f34-f13421cb1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_store = scio.loadmat('symbols_store_single.mat')\n",
    "symbols_store_mat = symbols_store['symbols_store']\n",
    "symbols_store_torch = torch.from_numpy(symbols_store_mat.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e633aa91-cded-4429-9500-f1a6ce7cafac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of the symbols is torch.int32\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data type of the symbols is {symbols_store_torch.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f7716fd-dd89-43a5-8f7d-66ef7222f968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the symbols is torch.Size([100000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the symbols is {symbols_store_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ecaa0232-5833-4d31-815a-dea111c9d2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  7,  6,  8,  0, 15, 12,  7,  1, 14], dtype=torch.int32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_store_torch[3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9785cecf-cbf2-4f21-903c-5154a1ebc30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat):\n",
    "        # convert into PyTorch tensors and remember them\n",
    "        self.algorithm_input_torch = algorithm_input_torch\n",
    "        self.algorithm_output_torch = algorithm_output_torch\n",
    "        self.main_channels_torch = main_channels_torch\n",
    "        self.symbols_store_torch = symbols_store_torch\n",
    "        \n",
    "    def __len__(self):\n",
    "        # this should return the size of the dataset\n",
    "        return len(self.algorithm_input_torch)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # this should return one sample from the dataset\n",
    "        algorithm_input_torch = self.algorithm_input_torch[idx,:]\n",
    "        algorithm_output_torch = self.algorithm_output_torch[idx,:]\n",
    "        main_channels_torch = self.main_channels_torch[idx,:,:]\n",
    "        symbols_store_torch = self.symbols_store_torch[idx,:]\n",
    "        return algorithm_input_torch, algorithm_output_torch, main_channels_torch, symbols_store_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c50918af-7aea-4a1a-a717-ae821e5e677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(algorithm_input_torch, algorithm_output_torch, main_channels_torch, symbols_store_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae911b39-4580-44fe-81de-864892143bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the dataset into train and remaining (val + test)\n",
    "train_set, remaining_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, split the remaining set into validation and test sets\n",
    "val_set, test_set = train_test_split(remaining_set, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47b372d6-28d2-4e1b-b2c7-4ecc3f46c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_set, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8017a9e-34ad-44fe-b5bf-75bedbb2f59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2425bdd-3cfc-4088-9f70-28db15772818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of batch feature is torch.Size([64, 70])\n",
      "shape of batch feature is torch.Size([64, 70])\n",
      "shape of batch feature is torch.Size([64, 10, 70])\n",
      "shape of batch feature is torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_alg_in_torch, batch_alg_out_torch, batch_main_chan_torch, batch_sym_torch = next(iter(train_loader))\n",
    "print(f'shape of batch feature is {batch_alg_in_torch.shape}')\n",
    "print(f'shape of batch feature is {batch_alg_out_torch.shape}')\n",
    "print(f'shape of batch feature is {batch_main_chan_torch.shape}')\n",
    "print(f'shape of batch feature is {batch_sym_torch.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d320b502-bb2e-499b-820a-987f8d708f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type of batch feature is torch.complex64\n",
      "data type of batch feature is torch.complex64\n",
      "data type of batch feature is torch.complex64\n",
      "data type of batch feature is torch.int32\n"
     ]
    }
   ],
   "source": [
    "batch_alg_in_torch, batch_alg_out_torch, batch_main_chan_torch, batch_sym_torch = next(iter(train_loader))\n",
    "print(f'data type of batch feature is {batch_alg_in_torch.dtype}')\n",
    "print(f'data type of batch feature is {batch_alg_out_torch.dtype}')\n",
    "print(f'data type of batch feature is {batch_main_chan_torch.dtype}')\n",
    "print(f'data type of batch feature is {batch_sym_torch.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "780273d1-805e-4803-ba7c-8323900786af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2, 70])\n",
      "torch.Size([64, 2, 70])\n",
      "torch.Size([64, 2, 10, 70])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([torch.real(batch_alg_in_torch).float(), torch.imag(batch_alg_in_torch).float()], dim=1).shape)\n",
    "print(torch.stack([torch.real(batch_alg_out_torch).float(), torch.imag(batch_alg_out_torch).float()], dim=1).shape)\n",
    "print(torch.stack([torch.real(batch_main_chan_torch).float(), torch.imag(batch_main_chan_torch).float()], dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4e0a58e-4083-4f92-a375-da27fcd11eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_to_interleaved_real(complex_signal):\n",
    "    real_part = complex_signal.real.to(dtype=torch.float32) \n",
    "    imag_part = complex_signal.imag.to(dtype=torch.float32) \n",
    "    interleaved_signal = torch.stack((real_part, imag_part), dim=2).reshape(complex_signal.shape[0], -1)\n",
    "    return interleaved_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b139f32b-ea12-4cea-ab0c-ed14e4e73447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0]).unsqueeze(0)\n",
    "b = torch.tensor([3.0, 4.0]).unsqueeze(0)\n",
    "\n",
    "test_tensor = a + 1j * b\n",
    "test_tensor = torch.cat((test_tensor, test_tensor, test_tensor), dim=0)\n",
    "test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73f6e8c0-d23c-4fb6-9022-c353bd3e5d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interleaved_tensor = complex_to_interleaved_real(test_tensor)\n",
    "interleaved_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa1c6bd0-3830-41ee-8f79-168a01a7530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleaved_real_to_complex(interleaved_signal):\n",
    "    signal_length = interleaved_signal.shape[1] // 2\n",
    "    real_part = interleaved_signal[:, 0::2]  # Extract even indices\n",
    "    imag_part = interleaved_signal[:, 1::2]  # Extract odd indices\n",
    "    complex_signal = torch.complex(real_part, imag_part)\n",
    "    return complex_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "269dd79e-6ed6-4981-bec7-3bd94312f14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.+3.j, 2.+4.j],\n",
       "        [1.+3.j, 2.+4.j],\n",
       "        [1.+3.j, 2.+4.j]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_to_complex = interleaved_real_to_complex(interleaved_tensor)\n",
    "back_to_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d85abef5-3c5a-474f-aee1-1761ae32f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_papr_complex(signal):\n",
    "    # Compute |x[n]|^2: Magnitude squared of the complex signal\n",
    "    power_signal = torch.abs(signal)**2\n",
    "    \n",
    "    # Peak power\n",
    "    peak_power_signal= torch.max(power_signal, dim=1).values\n",
    "\n",
    "    # Average power\n",
    "    avg_power_signal = torch.mean(power_signal, dim=1)\n",
    "\n",
    "    # PAPR\n",
    "    papr_signal = peak_power_signal / avg_power_signal\n",
    "    \n",
    "    return papr_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b9bb3f2-4dc6-4155-a8da-627445761686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def papr_loss(signal_going_out, signal_coming_in):\n",
    "    # Compute PAPR before and after\n",
    "    papr_going_out = compute_papr_complex(signal_going_out)  # Transformed signal\n",
    "    papr_coming_in = compute_papr_complex(signal_coming_in)  # Original signal\n",
    "\n",
    "    # Penalize only if PAPR after is greater than PAPR before\n",
    "    papr_diff = torch.relu(papr_going_out - papr_coming_in)\n",
    "    \n",
    "    return torch.mean(papr_diff), torch.mean(papr_going_out), torch.mean(papr_coming_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1d9eb3a-3856-4452-805e-ca119bfacdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_matlab(batch_alg_in_mat, batch_alg_out_mat, batch_nn_out, batch_main_chan_mat, batch_sym_mat):\n",
    "    \n",
    "    batch_alg_in_mat_real = matlab.double(batch_alg_in_mat.real.tolist())\n",
    "    batch_alg_in_mat_imag = matlab.double(batch_alg_in_mat.imag.tolist())\n",
    "\n",
    "    batch_alg_out_mat_real = matlab.double(batch_alg_out_mat.real.tolist())\n",
    "    batch_alg_out_mat_imag = matlab.double(batch_alg_out_mat.imag.tolist())\n",
    "\n",
    "    batch_nn_out_real = matlab.double(batch_nn_out.real.tolist())\n",
    "    batch_nn_out_imag = matlab.double(batch_nn_out.imag.tolist())\n",
    "\n",
    "    batch_main_chan_mat_real = matlab.double(batch_main_chan_mat.real.tolist())\n",
    "    batch_main_chan_mat_imag = matlab.double(batch_main_chan_mat.imag.tolist())\n",
    "\n",
    "    #batch_sym_mat = int(batch_sym_mat)\n",
    "    batch_sym_mat = matlab.uint32(batch_sym_mat.tolist())\n",
    "\n",
    "    return batch_alg_in_mat_real, batch_alg_in_mat_imag, batch_alg_out_mat_real, batch_alg_out_mat_imag, batch_nn_out_real, batch_nn_out_imag, batch_main_chan_mat_real, batch_main_chan_mat_imag, batch_sym_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a291c63c-0a4f-491e-a6bf-d7971da97ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ser(alg_in_real, alg_in_imag, alg_out_real, alg_out_imag, nn_out_real, nn_out_imag, main_chan_real, main_chan_imag, sym):\n",
    "    snr_db = 40\n",
    "    \n",
    "    alg_in = torch.complex(alg_in_real, alg_in_imag).float()\n",
    "    alg_out = torch.complex(alg_out_real, alg_out_imag).float()\n",
    "    nn_out = torch.complex(nn_out_real, nn_out_imag).float()\n",
    "    main_chan = torch.complex(main_chan_real, main_chan_imag).float()\n",
    "    \n",
    "    batch = main_chan.shape[0]\n",
    "    Mr = 10\n",
    "    Mt = 70\n",
    "    nf_initial = Mr / (Mt - Mr)\n",
    "    nf = torch.tensor(nf_initial, dtype=torch.float32)\n",
    "    M = 16\n",
    "    theta = 0.9\n",
    "    qam_power = torch.tensor(10, dtype=torch.float32)\n",
    "    \n",
    "    SNR = 10 ** (snr_db / 10)\n",
    "    sigma2n = 1 / SNR\n",
    "    \n",
    "    wn = torch.sqrt(torch.tensor(sigma2n / 2)) * (\n",
    "        torch.randn(batch, Mr) + 1j * torch.randn(batch, Mr)\n",
    "    )\n",
    "    \n",
    "    algorithm_input_reshaped = alg_in.view(batch, 1, -1)\n",
    "    algorithm_output_reshaped = alg_out.view(batch, 1, -1)\n",
    "    nn_output_reshaped = nn_out.view(batch, 1, -1)\n",
    "    \n",
    "    output_1 = torch.sum(main_chan * algorithm_input_reshaped, dim=2) + wn\n",
    "    output_1_sym = qamdemod(torch.sqrt(qam_power * nf) * output_1, M)\n",
    "    \n",
    "    output_2 = torch.sum(main_chan * algorithm_output_reshaped, dim=2) + wn\n",
    "    output_2_sym = qamdemod(torch.sqrt((qam_power * nf) / theta) * output_2, M)\n",
    "    \n",
    "    output_3 = torch.sum(main_chan * nn_output_reshaped, dim=2) + wn\n",
    "    output_3_sym = qamdemod(torch.sqrt((qam_power * nf) / theta) * output_3, M)\n",
    "    \n",
    "    ser_1 = torch.sum(output_1_sym != sym, dim=1) / Mr\n",
    "    ser_2 = torch.sum(output_2_sym != sym, dim=1) / Mr\n",
    "    ser_3 = torch.sum(output_3_sym != sym, dim=1) / Mr\n",
    "    \n",
    "    ser = torch.cat([ser_1.unsqueeze(1), ser_2.unsqueeze(1), ser_3.unsqueeze(1)], dim=1)\n",
    "    return ser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0c00e4fa-534d-4ae9-977d-c5cca46a7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ser_loss_matlab(batch_alg_in_mat, batch_alg_out_mat, batch_nn_out, batch_main_chan_mat, batch_sym_mat):\n",
    "\n",
    "    batch_alg_in_mat_real, batch_alg_in_mat_imag, batch_alg_out_mat_real, batch_alg_out_mat_imag, batch_nn_out_real, batch_nn_out_imag, batch_main_chan_mat_real, batch_main_chan_mat_imag , batch_sym_mat = prepare_for_matlab(batch_alg_in_mat, batch_alg_out_mat, batch_nn_out, batch_main_chan_mat, batch_sym_mat)\n",
    "    ser_mat = eng.calculate_ser(batch_alg_in_mat_real, batch_alg_in_mat_imag, batch_alg_out_mat_real, batch_alg_out_mat_imag, batch_nn_out_real, batch_nn_out_imag, batch_main_chan_mat_real, batch_main_chan_mat_imag , batch_sym_mat)\n",
    "    ser_torch = torch.tensor(ser_mat, dtype=torch.float32)\n",
    "    ser_diff = torch.relu(ser_torch[:,2] - ser_torch[:,1])\n",
    "    return torch.mean(ser_diff), torch.mean(ser_torch[:,2]), torch.mean(ser_torch[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6686224f-855d-4cc7-98f0-b1cca2aeae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ser_loss(batch_alg_in, batch_alg_out, batch_nn_out, batch_main_chan, batch_sym):\n",
    "\n",
    "    ser = calculate_ser(torch.real(batch_alg_in), torch.imag(batch_alg_in), torch.real(batch_alg_out), torch.imag(batch_alg_out), torch.real(batch_nn_out), torch.imag(batch_nn_out), torch.real(batch_main_chan), torch.imag(batch_main_chan), batch_sym)\n",
    "    #ser_torch = torch.tensor(ser, dtype=torch.float32)\n",
    "    ser_diff = torch.relu(ser[:,2] - ser[:,1])\n",
    "    return torch.mean(ser_diff), torch.mean(ser[:,2]), torch.mean(ser[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "950e67a3-1bfa-44b7-bce0-dadb29ef859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qammod(symbols, M=16):\n",
    "    assert M == 16, \"This function is specifically for 16-QAM.\"\n",
    "\n",
    "    # Define Gray-coded QAM-16 constellation mapping\n",
    "    real_values = torch.tensor([-3, -3, -3, -3, -1, -1, -1, -1, 3, 3, 3, 3, 1, 1, 1, 1], dtype=torch.float32)\n",
    "    imag_values = torch.tensor([3, 1, -3, -1, 3, 1, -3, -1, 3, 1, -3, -1, 3, 1, -3, -1], dtype=torch.float32)\n",
    "\n",
    "    # Flatten input before indexing\n",
    "    orig_shape = symbols.shape\n",
    "    symbols = symbols.view(-1)  # Flatten\n",
    "\n",
    "    # Map input symbols to corresponding constellation points\n",
    "    modulated_signal = torch.complex(real_values[symbols], imag_values[symbols])\n",
    "\n",
    "    return modulated_signal.view(orig_shape)  # Reshape back to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1f6009f4-3549-4206-9364-828a2917e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qamdemod(signal, M=16):\n",
    "    assert M == 16, \"This function is specifically for 16-QAM.\"\n",
    "\n",
    "    # Define Gray-coded QAM-16 constellation mapping\n",
    "    real_values = torch.tensor([-3, -3, -3, -3, -1, -1, -1, -1, 3, 3, 3, 3, 1, 1, 1, 1], dtype=torch.float32)\n",
    "    imag_values = torch.tensor([3, 1, -3, -1, 3, 1, -3, -1, 3, 1, -3, -1, 3, 1, -3, -1], dtype=torch.float32)\n",
    "\n",
    "    constellation = torch.complex(real_values, imag_values)\n",
    "\n",
    "    # Flatten input before processing\n",
    "    orig_shape = signal.shape\n",
    "    signal = signal.view(-1)  # Flatten\n",
    "\n",
    "    # Find the closest constellation point for each received symbol\n",
    "    distances = torch.abs(signal.unsqueeze(1) - constellation)\n",
    "    closest_indices = torch.argmin(distances, dim=1)\n",
    "\n",
    "    return closest_indices.view(orig_shape)  # Reshape back to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "559501bb-8ffc-474f-bb5a-c3564e193abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_use = torch.randint(0, 16, (64, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a41db608-b2da-4ba6-94ca-0f78a96065e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modulated_signal = qammod(data_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38721b-6ccb-4fb0-aa21-c161b8b7518c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b774ba35-59c5-4b37-b6ed-da4b3b58cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_indices = qamdemod(modulated_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "24e799de-f3d8-497a-b838-27756543effe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sym_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b3ed0d6-0000-48a5-9edb-95f5263a165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sym_torch_test = batch_sym_torch[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a84ece52-7316-4e89-86fd-d4e2f0a9c4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 13,  3,  3,  5,  6,  3,  9, 14,  0], dtype=torch.int32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sym_torch_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95cab56c-aa5d-4b29-9422-bd00ab42059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.9487\n",
    "b = 0.3162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8405efc3-4938-42f5-8bf2-a649be43a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_real = [a, -a, -a, b, -a, -a, b, -b, -b, -a]\n",
    "test_data_imag = [-b, -b, a, -a, -b, a, -b, -a, a, a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cd79f5c8-3ccf-49f8-8053-54d4ad562e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_real_torch = torch.tensor(test_data_real, dtype=torch.float32)\n",
    "test_data_imag_torch = torch.tensor(test_data_imag, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e588a44-d20a-4376-9a54-a0a057fdc9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_torch = torch.complex(test_data_real_torch, test_data_imag_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "65934ac4-06c2-4635-87e6-8c1fd7c76019",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = qamdemod(test_data_torch, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9e3a34fc-1ea4-40b0-924b-93e94c261b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 5, 6, 9, 5, 6, 9, 5, 6, 6])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fec95d-ec30-4260-ad01-3a6bc02e1c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc99ea5-50ef-442c-b73a-678651307dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a87bc07c-fcb2-4c90-9540-331aa8493bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_loss(nn_output, algorithm_output_mat_for_nn):\n",
    "    power_nn_output = torch.sum(torch.square(torch.abs(nn_output)),dim=1)\n",
    "    power_algorithm_output_mat_for_nn = torch.sum(torch.square(torch.abs(algorithm_output_mat_for_nn)),dim=1)\n",
    "    power_diff = torch.abs(power_nn_output - power_algorithm_output_mat_for_nn)\n",
    "    return torch.mean(power_diff), torch.mean(power_nn_output), torch.mean(power_algorithm_output_mat_for_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a754dea-63c7-4344-9687-3ac14e96c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand([64,10])\n",
    "b = torch.rand([64,10])\n",
    "c = torch.rand([64,10])\n",
    "d = torch.rand([64,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aada6516-b9c8-4454-bdbf-f26707d24204",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = torch.complex(a,b)\n",
    "cd = torch.complex(c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90f47d01-3eaa-468c-b04b-f5e409a541a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n,o = power_loss(ab,cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89bed10c-aad0-4a00-a374-aace06f497cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4290)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c40d03e6-68f6-4119-a729-aa663e1d60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9572bfd-89c2-4b84-a5c6-325114e6ac92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1719)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d7bf5eb-43c6-4816-add3-eb79fd60f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(nn_output, algorithm_output_mat_for_nn):\n",
    "    mse = torch.mean(torch.mean(torch.abs(torch.square(nn_output - algorithm_output_mat_for_nn)), dim=1))\n",
    "    return mse.round(decimals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8b270cb-34eb-4585-a9ff-0ed5adb416fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1719 and torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "c = mse_loss(a,b)\n",
    "print(f'{c:.4f} and {c.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f881ebf-f18f-43ec-959d-4e26e971182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.complex64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_alg_in_torch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a29d84b-be33-4a5f-9dbb-51ad6a306af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0009)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(complex_to_interleaved_real(batch_alg_in_torch), complex_to_interleaved_real(batch_alg_out_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a8d47df-d717-4943-b542-6b2000f1283b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(power_loss(complex_to_interleaved_real(batch_alg_in_torch), complex_to_interleaved_real(batch_alg_out_torch))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e5f2535-bf0a-4955-badf-a72dc790411f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0232, 0.0100, 0.0217, 0.0090, 0.0375, 0.0086, 0.0166, 0.0394, 0.0065,\n",
       "         0.0181, 0.0126, 0.0216, 0.0455, 0.0059, 0.0300, 0.0534, 0.0479, 0.0013,\n",
       "         0.0030, 0.0168, 0.0001, 0.0015, 0.0054, 0.0318, 0.0212, 0.0653, 0.0372,\n",
       "         0.0177, 0.0405, 0.0078, 0.0131, 0.0074, 0.0366, 0.0253, 0.0197, 0.0126,\n",
       "         0.0334, 0.0015, 0.0199, 0.0058, 0.0335, 0.0195, 0.0085, 0.0067, 0.0320,\n",
       "         0.0081, 0.0225, 0.0602, 0.0242, 0.0147, 0.0464, 0.0086, 0.0594, 0.0177,\n",
       "         0.0074, 0.0555, 0.0261, 0.0421, 0.0262, 0.0159, 0.0328, 0.0201, 0.0120,\n",
       "         0.0404]),\n",
       " tensor([0.9349, 1.0507, 0.9406, 1.1183, 0.7892, 1.2544, 0.9913, 0.7664, 1.2360,\n",
       "         1.3330, 1.0260, 0.9670, 0.7224, 1.1259, 0.8657, 0.6489, 0.6813, 1.1918,\n",
       "         1.1810, 0.9952, 1.1672, 1.1670, 1.1140, 0.8435, 0.9918, 0.4971, 0.8092,\n",
       "         1.0498, 0.8340, 1.0760, 1.2925, 1.0987, 0.7875, 0.9152, 0.9766, 1.0277,\n",
       "         0.8658, 1.1373, 0.9522, 1.1290, 0.8159, 0.9756, 1.2426, 1.0846, 0.8498,\n",
       "         1.2344, 0.9303, 0.5710, 0.9908, 1.0060, 1.6137, 1.0661, 0.5735, 0.9996,\n",
       "         1.0770, 0.6004, 0.9327, 0.7464, 0.8899, 0.9938, 0.8447, 0.9978, 1.0639,\n",
       "         0.7619]),\n",
       " tensor([0.9581, 1.0607, 0.9622, 1.1272, 0.8267, 1.2458, 1.0079, 0.8058, 1.2295,\n",
       "         1.3149, 1.0386, 0.9886, 0.7680, 1.1319, 0.8957, 0.7023, 0.7292, 1.1905,\n",
       "         1.1780, 1.0120, 1.1673, 1.1655, 1.1194, 0.8753, 1.0130, 0.5624, 0.8465,\n",
       "         1.0674, 0.8745, 1.0838, 1.2795, 1.1061, 0.8241, 0.9405, 0.9963, 1.0403,\n",
       "         0.8992, 1.1388, 0.9721, 1.1348, 0.8494, 0.9951, 1.2341, 1.0913, 0.8818,\n",
       "         1.2262, 0.9528, 0.6312, 1.0150, 1.0207, 1.5673, 1.0747, 0.6329, 1.0173,\n",
       "         1.0844, 0.6560, 0.9588, 0.7885, 0.9161, 1.0098, 0.8775, 1.0179, 1.0759,\n",
       "         0.8023]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_loss(batch_alg_in_torch, batch_alg_out_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "211817a2-16b6-431c-82bb-c43a199227a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = torch.complex(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5d4cd-c7e9-487c-b2b1-3c66e4c5aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc20844b-3f3f-4077-a53d-4d5a6d108bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.2460)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.square(torch.abs(ab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0c39b513-2949-455b-af5f-6f875f10b991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4149, -0.0350, -0.1526,  0.0091, -0.1886, -0.2704,  0.1865, -0.1763,\n",
       "         -0.1867,  0.0612]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc68ce-a834-46a9-acda-040ccc8b2023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50c9a202-ef06-450d-8f24-5b45a3b2833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSIModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CSIModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bnconv1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bnconv2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "        self.bnconv3 = nn.BatchNorm2d(1)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.linear1 = nn.Linear(700, 280)\n",
    "        self.bnlin1 = nn.BatchNorm1d(280)\n",
    "        \n",
    "        self.linear2 = nn.Linear(280, 140)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(1)\n",
    "        x = F.relu(self.bnconv1(self.conv1(x)))\n",
    "        x = F.relu(self.bnconv2(self.conv2(x)))\n",
    "        x = F.relu(self.bnconv3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.bnlin1(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f172b873-cecf-4482-8078-3ca48377dd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 140])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_1 = CSIModel()(torch.rand([64,2,10,70]))\n",
    "test_output_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f46d8be-f848-4eff-8bcf-56d92253c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.rand([32,8,64,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b687240-a841-459d-b66e-03397d4a6919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.adaptive_avg_pool2d(test_tensor,(1,1)).squeeze(-1).squeeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6118a666-2b05-438a-b0fd-1a6bf211e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SignalModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(140, 70)\n",
    "        self.bnlin1 = nn.BatchNorm1d(70)\n",
    "\n",
    "        self.linear2 = nn.Linear(70, 14)\n",
    "        self.bnlin2 = nn.BatchNorm1d(14)\n",
    "\n",
    "        self.linear3 = nn.Linear(14, 70)\n",
    "        self.bnlin3 = nn.BatchNorm1d(70)\n",
    "\n",
    "        self.linear4 = nn.Linear(70, 140)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(1)\n",
    "        x = F.relu(self.bnlin1(self.linear1(x)))\n",
    "        x = F.relu(self.bnlin2(self.linear2(x)))\n",
    "        x = F.relu(self.bnlin3(self.linear3(x)))\n",
    "        x = self.linear4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c3feb30-605e-4825-b759-28012418e7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 140])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_2 = SignalModel()(torch.rand([64,140]))\n",
    "test_output_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44034148-83aa-4850-8664-96eb20b0ded3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 280])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = torch.cat([test_output_1, test_output_2], dim=1)\n",
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5466786c-718c-4e14-8f8b-6d95defefb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        \n",
    "        self.csimodel = CSIModel()\n",
    "        self.signalmodel = SignalModel()\n",
    "\n",
    "        \n",
    "        self.linear1 = nn.Linear(280,140)\n",
    "        self.bnlin1 = nn.BatchNorm1d(280)\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        #x = x.unsqueeze(1)\n",
    "        x1 = self.csimodel(x1)\n",
    "        x2 = self.signalmodel(x2)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = self.linear1(self.bnlin1(x))\n",
    "  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4c5b24b-b561-4ae2-be90-4053d43d8a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 140])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_final= CombinedModel()(torch.rand([64,2,10,70]), torch.rand([64,140]))\n",
    "test_output_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0b8f3f7a-be21-46f2-8754-23dc991b56ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10] T Loss:1.4263 PAPR_dff: 0.8752 NN_PAPR: 3.0062 Alg_PAPR: 2.1361 SER_dff: 0.0797 NN_SER: 0.9328 Alg_SER: 0.8609 Power_dff: 0.8752 NN_Power: 2.4763 Alg_Power: 1.0154 LR is 0.001: 100%|█| 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has completed epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10] V Loss:0.6565 PAPR_dff: 0.6828 NN_PAPR: 2.7743 Alg_PAPR: 2.0916 SER_dff: 0.0688 NN_SER: 0.9500 Alg_SER: 0.8938 Power_dff: 0.6828 NN_Power: 1.9558 Alg_Power: 1.0805: 100%|█| 157/157 [00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.6565 at Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/10] T Loss:0.4336 PAPR_dff: 0.1868 NN_PAPR: 2.2535 Alg_PAPR: 2.1221 SER_dff: 0.0922 NN_SER: 0.9406 Alg_SER: 0.8531 Power_dff: 0.1868 NN_Power: 1.7116 Alg_Power: 1.0067 LR is 0.001: 100%|█| 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has completed epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/10] V Loss:0.2943 PAPR_dff: 0.2441 NN_PAPR: 2.2875 Alg_PAPR: 2.0916 SER_dff: 0.0750 NN_SER: 0.9250 Alg_SER: 0.8625 Power_dff: 0.2441 NN_Power: 1.3400 Alg_Power: 1.0805: 100%|█| 157/157 [00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.2943 at Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/10] T Loss:0.1985 PAPR_dff: 0.0551 NN_PAPR: 1.8590 Alg_PAPR: 2.1104 SER_dff: 0.0531 NN_SER: 0.9422 Alg_SER: 0.8937 Power_dff: 0.0551 NN_Power: 1.4133 Alg_Power: 1.0374 LR is 0.001: 100%|█| 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has completed epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/10] V Loss:0.2051 PAPR_dff: 0.0288 NN_PAPR: 1.8384 Alg_PAPR: 2.0916 SER_dff: 0.0688 NN_SER: 0.9187 Alg_SER: 0.8563 Power_dff: 0.0288 NN_Power: 1.1216 Alg_Power: 1.0805: 100%|█| 157/157 [00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.2051 at Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/10] T Loss:0.1332 PAPR_dff: 0.0379 NN_PAPR: 1.6923 Alg_PAPR: 2.0927 SER_dff: 0.0781 NN_SER: 0.9578 Alg_SER: 0.8859 Power_dff: 0.0379 NN_Power: 1.1997 Alg_Power: 1.0343 LR is 0.001: 100%|█| 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has completed epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/10] V Loss:0.1165 PAPR_dff: 0.0983 NN_PAPR: 1.7893 Alg_PAPR: 2.0916 SER_dff: 0.0625 NN_SER: 0.9250 Alg_SER: 0.8687 Power_dff: 0.0983 NN_Power: 1.3204 Alg_Power: 1.0805: 100%|█| 157/157 [00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.1165 at Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/10] T Loss:0.1096 PAPR_dff: 0.0521 NN_PAPR: 1.6987 Alg_PAPR: 2.0605 SER_dff: 0.0594 NN_SER: 0.9484 Alg_SER: 0.8937 Power_dff: 0.0521 NN_Power: 1.1555 Alg_Power: 1.0621 LR is 0.001: 100%|█| 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has completed epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/10] V Loss:0.1471 PAPR_dff: 0.1290 NN_PAPR: 1.7412 Alg_PAPR: 2.0916 SER_dff: 0.0938 NN_SER: 0.9375 Alg_SER: 0.8500 Power_dff: 0.1290 NN_Power: 1.2808 Alg_Power: 1.0805: 100%|█| 157/157 [00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Epoch: 1 without improvement\n",
      "Current Validation Loss is: 0.1471 at Epoch: 5\n",
      "Best Validation Loss remains: 0.1165 at Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/10] T Loss:0.0992 PAPR_dff: 0.0326 NN_PAPR: 1.5995 Alg_PAPR: 2.1358 SER_dff: 0.0562 NN_SER: 0.9219 Alg_SER: 0.8750 Power_dff: 0.0326 NN_Power: 1.1577 Alg_Power: 1.0085 LR is 0.001: 100%|█| 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has completed epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/10] V Loss:0.1497 PAPR_dff: 0.0000 NN_PAPR: 1.5803 Alg_PAPR: 2.0916 SER_dff: 0.0750 NN_SER: 0.9312 Alg_SER: 0.8562 Power_dff: 0.0000 NN_Power: 1.0556 Alg_Power: 1.0805: 100%|█| 157/157 [00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Epoch: 2 without improvement\n",
      "Current Validation Loss is: 0.1497 at Epoch: 6\n",
      "Best Validation Loss remains: 0.1165 at Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/10] T Loss:0.0834 PAPR_dff: 0.0238 NN_PAPR: 1.7538 Alg_PAPR: 2.1012 SER_dff: 0.0688 NN_SER: 0.9422 Alg_SER: 0.8766 Power_dff: 0.0238 NN_Power: 1.1277 Alg_Power: 1.0104 LR is 0.001: 100%|█| 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has completed epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/10] V Loss:0.0720 PAPR_dff: 0.0000 NN_PAPR: 1.6828 Alg_PAPR: 2.0916 SER_dff: 0.0625 NN_SER: 0.9438 Alg_SER: 0.8813 Power_dff: 0.0000 NN_Power: 1.0606 Alg_Power: 1.0805: 100%|█| 157/157 [00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.0720 at Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/10] T Loss:0.0683 PAPR_dff: 0.0085 NN_PAPR: 1.6112 Alg_PAPR: 2.1236 SER_dff: 0.0766 NN_SER: 0.9438 Alg_SER: 0.8703 Power_dff: 0.0085 NN_Power: 1.0318 Alg_Power: 1.0165 LR is 0.001: 100%|█| 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has completed epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/10] V Loss:0.0636 PAPR_dff: 0.0000 NN_PAPR: 1.5354 Alg_PAPR: 2.0916 SER_dff: 0.0750 NN_SER: 0.9562 Alg_SER: 0.8813 Power_dff: 0.0000 NN_Power: 1.0438 Alg_Power: 1.0805: 100%|█| 157/157 [00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.0636 at Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/10] T Loss:0.0496 PAPR_dff: 0.0000 NN_PAPR: 1.5125 Alg_PAPR: 2.0842 SER_dff: 0.0781 NN_SER: 0.9422 Alg_SER: 0.8703 Power_dff: 0.0000 NN_Power: 1.0122 Alg_Power: 1.0370 LR is 0.001: 100%|█| 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has completed epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/10] V Loss:0.0559 PAPR_dff: 0.0000 NN_PAPR: 1.4727 Alg_PAPR: 2.0916 SER_dff: 0.0563 NN_SER: 0.9375 Alg_SER: 0.8812 Power_dff: 0.0000 NN_Power: 1.0753 Alg_Power: 1.0805: 100%|█| 157/157 [00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.0559 at Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/10] T Loss:0.0465 PAPR_dff: 0.0000 NN_PAPR: 1.4086 Alg_PAPR: 2.0901 SER_dff: 0.0719 NN_SER: 0.9328 Alg_SER: 0.8641 Power_dff: 0.0000 NN_Power: 1.0178 Alg_Power: 1.0573 LR is 0.001: 100%|█| "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has completed epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/10] V Loss:0.0473 PAPR_dff: 0.0000 NN_PAPR: 1.3860 Alg_PAPR: 2.0916 SER_dff: 0.0500 NN_SER: 0.9563 Alg_SER: 0.9062 Power_dff: 0.0000 NN_Power: 1.1113 Alg_Power: 1.0805: 100%|█| 157/157 [00:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss is now: 0.0473 at Epoch: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CombinedModel().to(device)\n",
    "\n",
    "# Define the loss functions\n",
    "loss = torch.nn.MSELoss()  # For classification\n",
    "\n",
    "# Define an optimizer (both for the encoder and the decoder!)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.01)  # Learning rate decay scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=2)\n",
    "\n",
    "# Variables for early stopping and best parameters\n",
    "best_loss = float('inf')\n",
    "patience_limit = 2\n",
    "\n",
    "\n",
    "best_model = None\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "alpha = 0.4\n",
    "beta = 0.4\n",
    "gamma = 0.2\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    progress_bar_train = tqdm(enumerate(train_loader), total=len(train_loader), ncols=200)\n",
    "    for index, (algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat) in progress_bar_train:\n",
    "        # Forward pass\n",
    "        #algorithm_output_mat_for_nn = (batch_complex_autocorrelation(algorithm_output_mat)).to(device)\n",
    "        main_channels_mat_for_nn = torch.stack([torch.real(main_channels_mat).float(), torch.imag(main_channels_mat).float()], dim=1).to(device)\n",
    "        algorithm_input_mat_for_nn = (complex_to_interleaved_real(algorithm_input_mat)).to(device)\n",
    "        algorithm_output_mat_for_nn = (complex_to_interleaved_real(algorithm_output_mat)).to(device)\n",
    "\n",
    "        \n",
    "        nn_output = model(main_channels_mat_for_nn, algorithm_input_mat_for_nn)\n",
    "        \n",
    "        # Calculate loss\n",
    "        mse_train_loss = loss(nn_output, algorithm_output_mat_for_nn)\n",
    "\n",
    "        nn_output_control =  interleaved_real_to_complex(nn_output)\n",
    "        algorithm_output_mat_for_nn_control =  interleaved_real_to_complex(algorithm_output_mat_for_nn)\n",
    "        \n",
    "        papr_diff, nn_papr, alg_papr  = papr_loss(nn_output_control, algorithm_output_mat_for_nn_control)\n",
    "        ser_diff, nn_ser, alg_ser = ser_loss(algorithm_input_mat, algorithm_output_mat_for_nn_control.cpu(), nn_output_control.cpu(), main_channels_mat, symbols_store_mat)\n",
    "        power_diff, nn_power, alg_power = power_loss(nn_output_control, algorithm_output_mat_for_nn_control)\n",
    "        \n",
    "        train_loss = alpha*papr_diff + beta*ser_diff + gamma*power_diff\n",
    "        #train_loss = mse_train_loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_train_loss += train_loss.item()\n",
    "        avg_train_loss = running_train_loss / (index + 1)\n",
    "\n",
    "        # Get current learning rate from the optimizer\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Print metrics\n",
    "        #progress_bar_train.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] MSELos:{avg_train_loss1:.4f} MSEWeig{mse_weight:.2f} CELos:{avg_train_loss2:.4f} CEWeig{ce_weight:.2f} TrLos:{avg_train_loss:.4f} Tr.Acc: {avg_train_acc*100:.2f}%')\n",
    "        progress_bar_train.set_description(f\" Epoch [{epoch + 1}/{EPOCHS}] T Loss:{avg_train_loss:.4f} PAPR_dff: {papr_diff:.4f} NN_PAPR: {nn_papr:.4f} Alg_PAPR: {alg_papr:.4f} SER_dff: {ser_diff:.4f} NN_SER: {nn_ser:.4f} Alg_SER: {alg_ser:.4f} Power_dff: {papr_diff:.4f} NN_Power: {nn_power:.4f} Alg_Power: {alg_power:.4f} LR is {current_lr}\")\n",
    "    \n",
    "    #train_losses.append(avg_train_loss)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    print(f\"Training has completed epoch {epoch+1}\")\n",
    "    \n",
    "    # Validation loop\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    progress_bar_val = tqdm(enumerate(val_loader), total=len(val_loader), ncols=200)\n",
    "    for index, (algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat) in progress_bar_val:\n",
    "        \n",
    "        #algorithm_output_mat_for_nn = (batch_complex_autocorrelation(algorithm_output_mat)).to(device)\n",
    "        main_channels_mat_for_nn = torch.stack([torch.real(main_channels_mat).float(), torch.imag(main_channels_mat).float()], dim=1).to(device)\n",
    "        algorithm_input_mat_for_nn = (complex_to_interleaved_real(algorithm_input_mat)).to(device)\n",
    "        algorithm_output_mat_for_nn = (complex_to_interleaved_real(algorithm_output_mat)).to(device)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            nn_output = model(main_channels_mat_for_nn, algorithm_input_mat_for_nn)\n",
    "\n",
    "            # Calculate losses\n",
    "            mse_val_loss = loss(nn_output, algorithm_output_mat_for_nn)\n",
    "\n",
    "            nn_output_control =  interleaved_real_to_complex(nn_output)\n",
    "            algorithm_output_mat_for_nn_control =  interleaved_real_to_complex(algorithm_output_mat_for_nn)\n",
    "\n",
    "            papr_diff, nn_papr, alg_papr  = papr_loss(nn_output_control, algorithm_output_mat_for_nn_control)\n",
    "            ser_diff, nn_ser, alg_ser = ser_loss(algorithm_input_mat, algorithm_output_mat_for_nn_control.cpu(), nn_output_control.cpu(), main_channels_mat, symbols_store_mat)\n",
    "            power_diff, nn_power, alg_power = power_loss(nn_output_control, algorithm_output_mat_for_nn_control)\n",
    "        \n",
    "            val_loss = alpha*papr_diff + beta*ser_diff + gamma*power_diff\n",
    "\n",
    "            # Update running loss\n",
    "            running_val_loss += val_loss.item()\n",
    "            avg_val_loss = running_val_loss / (index + 1)\n",
    "\n",
    "    \n",
    "            progress_bar_val.set_description(f\" Epoch [{epoch + 1}/{EPOCHS}] V Loss:{avg_val_loss:.4f} PAPR_dff: {papr_diff:.4f} NN_PAPR: {nn_papr:.4f} Alg_PAPR: {alg_papr:.4f} SER_dff: {ser_diff:.4f} NN_SER: {nn_ser:.4f} Alg_SER: {alg_ser:.4f} Power_dff: {papr_diff:.4f} NN_Power: {nn_power:.4f} Alg_Power: {alg_power:.4f}\")\n",
    "    \n",
    "    #val_losses.append(avg_val_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    scheduler.step(running_val_loss)\n",
    "\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:  # Now checking for the best accuracy\n",
    "        best_loss = avg_val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        best_train_loss = avg_train_loss\n",
    "        patience_ = 0\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "        print(f\"Best Validation Loss is now: {best_loss:.4f} at Epoch: {best_epoch}\")\n",
    "    else:\n",
    "        patience_ += 1\n",
    "        print(f\"This is Epoch: {patience_} without improvement\")\n",
    "        print(f\"Current Validation Loss is: {avg_val_loss:.4f} at Epoch: {epoch+1}\")\n",
    "        print(f\"Best Validation Loss remains: {best_loss:.4f} at Epoch: {best_epoch}\")\n",
    "        if patience_ > patience_limit:  # Patience limit before stopping\n",
    "            print(\"Early stopping triggered! Restoring best model weights.\")\n",
    "            print(f\"Best Validation Loss was: {best_loss:.4f} at Epoch: {best_epoch}\")\n",
    "            break\n",
    "\n",
    "best_model = model.cpu()\n",
    "best_model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "830b9432-9787-4c17-ab5c-d7d6378506d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch [10/10] Te Loss:0.0146 PAPR_dff: 0.0000 SER_dff: 0.0438: 100%|████████████████████████████████████████████████| 157/157 [00:02<00:00, 59.42it/s]"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "running_test_loss = 0.0\n",
    "\n",
    "\n",
    "progress_bar_test = tqdm(enumerate(test_loader), total=len(test_loader), ncols=150)\n",
    "for index, (algorithm_input_mat, algorithm_output_mat, main_channels_mat, symbols_store_mat) in progress_bar_test:\n",
    "        \n",
    "    main_channels_mat_for_nn = torch.stack([torch.real(main_channels_mat).float(), torch.imag(main_channels_mat).float()], dim=1)\n",
    "    algorithm_input_mat_for_nn = (complex_to_interleaved_real(algorithm_input_mat))\n",
    "    algorithm_output_mat_for_nn = (complex_to_interleaved_real(algorithm_output_mat))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            \n",
    "        nn_output = best_model(main_channels_mat_for_nn, algorithm_input_mat_for_nn)\n",
    "\n",
    "        # Calculate losses\n",
    "        test_loss = loss(nn_output, algorithm_output_mat_for_nn)\n",
    "\n",
    "        # Update running loss\n",
    "        running_test_loss += test_loss.item()\n",
    "            \n",
    "        avg_test_loss = running_test_loss / (index + 1)\n",
    "\n",
    "        nn_output_control =  interleaved_real_to_complex(nn_output)\n",
    "        algorithm_output_mat_for_nn_control =  interleaved_real_to_complex(algorithm_output_mat_for_nn)\n",
    "        \n",
    "        papr_diff, nn_papr, alg_papr = papr_loss(nn_output_control, algorithm_output_mat_for_nn_control)\n",
    "        ser_diff, nn_ser, alg_ser = ser_loss(algorithm_input_mat, algorithm_output_mat_for_nn_control, nn_output_control, main_channels_mat, symbols_store_mat)\n",
    "\n",
    "        progress_bar_test.set_description(f'Epoch [{epoch + 1}/{EPOCHS}] Te Loss:{avg_test_loss:.4f} PAPR_dff: {papr_diff:.4f} SER_dff: {ser_diff:.4f}')\n",
    "\n",
    "        \n",
    "        if index < 1:\n",
    "            total_nn_out = nn_output_control\n",
    "            total_alg_in = algorithm_input_mat\n",
    "            total_alg_out = algorithm_output_mat_for_nn_control\n",
    "            total_main_channels = main_channels_mat\n",
    "            total_symbols = symbols_store_mat\n",
    "        else:\n",
    "            total_nn_out = torch.cat([total_nn_out, nn_output_control], dim=0, out=None)\n",
    "            total_alg_in = torch.cat([total_alg_in, algorithm_input_mat], dim=0, out=None)\n",
    "            total_alg_out = torch.cat([total_alg_out, algorithm_output_mat_for_nn_control], dim=0, out=None)\n",
    "            total_main_channels = torch.cat([total_main_channels, main_channels_mat], dim=0, out=None)\n",
    "            total_symbols = torch.cat([total_symbols, symbols_store_mat], dim=0, out=None)\n",
    "\n",
    "test_losses.append(avg_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "66f020a4-b573-4f70-8d66-dab90fc0de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_alg_in_real, total_alg_in_imag, total_alg_out_real, total_alg_out_imag, total_nn_out_real, total_nn_out_imag, total_main_channels_real, total_main_channels_imag, total_symbols = prepare_for_matlab(total_alg_in, total_alg_out, total_nn_out, total_main_channels, total_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6f0d677f-c655-4a98-9636-1955d4cdc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all variables in a dictionary\n",
    "savemat(\"output_from_pytorch.mat\", {\n",
    "    \"total_alg_in_real\": total_alg_in_real,\n",
    "    \"total_alg_in_imag\": total_alg_in_imag,\n",
    "    \"total_alg_out_real\": total_alg_out_real,\n",
    "    \"total_alg_out_imag\": total_alg_out_imag,\n",
    "    \"total_nn_out_real\": total_nn_out_real,\n",
    "    \"total_nn_out_imag\": total_nn_out_imag,\n",
    "    \"total_main_channels_real\": total_main_channels_real,\n",
    "    \"total_main_channels_imag\": total_main_channels_imag,\n",
    "    \"total_symbols\": total_symbols\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e49995-da2d-4b44-ad81-59217a411746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
